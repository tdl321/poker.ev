# RAG Architecture - Poker Advisor System

## Overview

The poker.ev AI advisor uses a **Retrieval-Augmented Generation (RAG)** system to provide contextual poker advice. This document explains how the UI, LLM, streaming, and RAG components work together.

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Interface                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ChatPanel (poker_ev/gui/chat/chat_panel.py)             â”‚  â”‚
â”‚  â”‚  - Message display & input                                â”‚  â”‚
â”‚  â”‚  - Scroll handling                                        â”‚  â”‚
â”‚  â”‚  - Event handling                                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Poker Advisor Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PokerAdvisor (poker_ev/llm/poker_advisor.py)            â”‚  â”‚
â”‚  â”‚  - Orchestrates RAG pipeline                             â”‚  â”‚
â”‚  â”‚  - Combines context sources                              â”‚  â”‚
â”‚  â”‚  - Manages streaming responses                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                â”‚                â”‚
           â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Game Context    â”‚ â”‚ RAG Vector  â”‚ â”‚ Ollama LLM      â”‚
â”‚  Provider        â”‚ â”‚ Store       â”‚ â”‚ Client          â”‚
â”‚                  â”‚ â”‚             â”‚ â”‚                 â”‚
â”‚  - Hand state    â”‚ â”‚ - Poker KB  â”‚ â”‚ - phi3:mini     â”‚
â”‚  - Position      â”‚ â”‚ - Semantic  â”‚ â”‚ - Streaming     â”‚
â”‚  - Pot odds      â”‚ â”‚   search    â”‚ â”‚ - HTTP API      â”‚
â”‚  - Opponents     â”‚ â”‚ - Embeddingsâ”‚ â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### 1. UI Layer - Chat Panel

**Location:** `poker_ev/gui/chat/`

#### ChatPanel (`chat_panel.py`)
Main chat interface component that manages the user experience.

**Key Features:**
- Message history display
- User input field
- Typing indicators
- Auto-scrolling
- Event handling (keyboard, mouse)

**Flow:**
```python
User types message â†’ ChatPanel.handle_event()
                  â†’ on_message_send callback
                  â†’ PygameGUI._handle_chat_message()
```

#### MessageRenderer (`message_renderer.py`)
Renders chat messages with proper formatting and word wrapping.

**Features:**
- User messages (green/right-aligned)
- AI responses (yellow/left-aligned)
- Word wrapping for long text
- Timestamp display

#### ChatInput (`chat_input.py`)
Handles text input with cursor positioning and editing.

**Features:**
- Text editing with cursor
- Backspace/delete support
- Enter to submit
- Visual feedback

#### ScrollHandler (`scroll_handler.py`)
Manages scrolling through chat history.

**Features:**
- Mouse wheel scrolling
- Auto-scroll to bottom on new messages
- Scroll bar visualization

---

### 2. Poker Advisor - RAG Orchestration

**Location:** `poker_ev/llm/poker_advisor.py`

The `PokerAdvisor` class orchestrates the entire RAG pipeline.

#### Key Methods

##### `get_advice_stream(user_query, game_state, use_rag=True)`
Streaming advice generation with RAG.

**Pipeline:**
```python
1. Build System Prompt
   â†“
2. Add Game Context (if available)
   â”œâ”€ Get current hand state
   â”œâ”€ Player position & cards
   â”œâ”€ Pot odds calculation
   â””â”€ Opponent information
   â†“
3. Add RAG Context (if enabled)
   â”œâ”€ Embed user query
   â”œâ”€ Search vector store (k=2 chunks)
   â””â”€ Format retrieved knowledge
   â†“
4. Construct Messages
   â”œâ”€ System message (context)
   â””â”€ User message (query)
   â†“
5. Stream from Ollama
   â””â”€ Yield chunks as they arrive
```

**Code Example:**
```python
def get_advice_stream(self, user_query, game_state=None, use_rag=True):
    # 1. System prompt
    context_parts = [self.SYSTEM_PROMPT]

    # 2. Game context
    if game_state and self.game_context_provider:
        game_context = self.game_context_provider.get_full_context()
        context_parts.append(f"\nCurrent Game State:\n{game_context}")

    # 3. RAG context
    if use_rag:
        rag_context = self.vector_store.search_as_context(user_query, k=2)
        context_parts.append(f"\nRelevant Poker Strategy:\n{rag_context}")

    # 4. Build messages
    system_msg = '\n'.join(context_parts)
    messages = [
        {'role': 'system', 'content': system_msg},
        {'role': 'user', 'content': user_query}
    ]

    # 5. Stream response
    for chunk in self.ollama.stream_chat(messages):
        yield chunk
```

---

### 3. Game Context Provider

**Location:** `poker_ev/llm/game_context.py`

Converts internal game state into LLM-friendly natural language.

#### Key Features

**Hand State:**
```
============================================================
CURRENT HAND
============================================================

ğŸƒ YOUR CARDS: Aâ™  Kâ™ 
ğŸ“ POSITION: Button (BTN)

ğŸ¯ PHASE: PRE-FLOP

ğŸ’° POT: $15
ğŸ’µ YOUR CHIPS: $1000
ğŸ“¢ TO CALL: $10
ğŸ“Š MIN RAISE: $10
```

**Opponent Profiling:**
```
OPPONENTS
============================================================

Player 1 (Call Agent (always calls)):
  Position: Small Blind (SB), $990, BET $5, ACTIVE

Player 3 (Aggressive Agent (raises often)):
  Position: Big Blind (BB), $985, BET $10, ACTIVE
```

**Pot Odds Calculation:**
```
Pot Odds Analysis:
  â€¢ You need to call: $10
  â€¢ Current pot: $15
  â€¢ Total pot after call: $25
  â€¢ Pot odds: 2.5:1
  â€¢ Break-even equity needed: 40.0%
```

---

### 4. RAG Vector Store

**Location:** `poker_ev/rag/`

#### Vector Store Architecture

**Two implementations:**
1. **PineconePokerStore** - Cloud vector database (requires API key)
2. **InMemoryPokerStore** - Local fallback (used when Pinecone unavailable)

#### Knowledge Base

**Location:** `poker_ev/rag/knowledge_base/`

**Documents:**
- `hand_rankings.md` - Hand strength and ranges
- `position_strategy.md` - Position-based play
- `pot_odds.md` - Mathematical concepts
- `opponent_profiling.md` - Reading opponents

#### Document Loading Pipeline

```python
# DocumentLoader (rag/document_loader.py)
1. Load markdown files from knowledge_base/
   â†“
2. Split into chunks (500 chars, 100 overlap)
   â†“
3. Generate embeddings (sentence-transformers)
   â†“
4. Store in vector database with metadata
```

#### Semantic Search

**Query Process:**
```python
user_query = "Should I call with pocket jacks?"
                    â†“
1. Embed query with same model
                    â†“
2. Cosine similarity search in vector store
                    â†“
3. Retrieve top-k most relevant chunks (k=2)
                    â†“
4. Format as context for LLM:

"""
Relevant Poker Strategy:

[Chunk 1 - Score: 0.89]
Premium pairs like JJ-AA are strong hands...

[Chunk 2 - Score: 0.82]
From middle position, consider 3-betting...
"""
```

---

### 5. Ollama LLM Client

**Location:** `poker_ev/llm/ollama_client.py`

Interface to local Ollama service for LLM inference.

#### Configuration

```python
OllamaClient(
    base_url="http://localhost:11434",  # Local Ollama server
    model="phi3:mini",                   # 2.2GB model
    temperature=0.7,                     # Response creativity
    timeout=120                          # Request timeout
)
```

#### Streaming Chat

**HTTP Streaming:**
```python
def stream_chat(self, messages, temperature=None, max_tokens=None):
    payload = {
        "model": self.model,
        "messages": messages,
        "stream": True,  # Enable streaming
        "options": {
            "temperature": temperature or self.temperature
        }
    }

    # Stream response chunks
    response = requests.post(
        f"{self.base_url}/api/chat",
        json=payload,
        stream=True  # HTTP streaming
    )

    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if 'message' in data:
                content = data['message'].get('content', '')
                if content:
                    yield content  # Stream to caller
```

**Why Streaming?**
- **Better UX:** Show response as it's generated
- **Perceived speed:** User sees output immediately
- **Responsiveness:** Game doesn't freeze during generation

---

## Complete Data Flow

### Example: User asks "Should I call with pocket jacks?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER INPUT                                               â”‚
â”‚    User types in ChatPanel: "Should I call with JJ?"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. EVENT HANDLING                                           â”‚
â”‚    ChatPanel.on_message_send() â†’ callback                  â”‚
â”‚    PygameGUI._handle_chat_message()                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. CONTEXT GATHERING (Parallel)                            â”‚
â”‚                                                              â”‚
â”‚    A. Game Context Provider:                                â”‚
â”‚       - Current hand: Jâ™  Jâ™£                                â”‚
â”‚       - Position: Button (BTN)                              â”‚
â”‚       - Pot: $25, To call: $10                             â”‚
â”‚       - Opponent bets: Player 3 raised to $20              â”‚
â”‚                                                              â”‚
â”‚    B. RAG Vector Store:                                     â”‚
â”‚       - Embed query: "Should I call with JJ?"              â”‚
â”‚       - Search knowledge base                               â”‚
â”‚       - Retrieve: hand_rankings.md (JJ strategy)           â”‚
â”‚       - Retrieve: position_strategy.md (button play)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. PROMPT CONSTRUCTION                                      â”‚
â”‚                                                              â”‚
â”‚    System Prompt:                                           â”‚
â”‚    "You are a helpful poker advisor..."                    â”‚
â”‚                                                              â”‚
â”‚    + Game State:                                            â”‚
â”‚    "ğŸƒ YOUR CARDS: Jâ™  Jâ™£                                   â”‚
â”‚     ğŸ“ POSITION: Button (BTN)                              â”‚
â”‚     ğŸ’° POT: $25, TO CALL: $10..."                          â”‚
â”‚                                                              â”‚
â”‚    + RAG Context:                                           â”‚
â”‚    "Relevant Strategy:                                      â”‚
â”‚     JJ is a strong hand but vulnerable to overs..."        â”‚
â”‚                                                              â”‚
â”‚    User Query:                                              â”‚
â”‚    "Should I call with JJ?"                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. LLM STREAMING                                            â”‚
â”‚                                                              â”‚
â”‚    Thread 1 (Background):                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ OllamaClient.stream_chat()                 â”‚          â”‚
â”‚    â”‚   â†“                                         â”‚          â”‚
â”‚    â”‚ HTTP POST to localhost:11434/api/chat     â”‚          â”‚
â”‚    â”‚   â†“                                         â”‚          â”‚
â”‚    â”‚ phi3:mini generates response               â”‚          â”‚
â”‚    â”‚   â†“                                         â”‚          â”‚
â”‚    â”‚ Stream chunks: ["With", " pocket", " jacks"]â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â”‚    Thread 2 (Main - Game Loop):                            â”‚
â”‚    - Game continues running at 60 FPS                      â”‚
â”‚    - User can still interact with table                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. RESPONSE DISPLAY                                         â”‚
â”‚                                                              â”‚
â”‚    ChatPanel displays:                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚    â”‚ You: Should I call with JJ?              â”‚            â”‚
â”‚    â”‚                                  15:24   â”‚            â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚    â”‚ Poker Advisor: With pocket jacks from   â”‚            â”‚
â”‚    â”‚ the button, this is a strong hand.      â”‚            â”‚
â”‚    â”‚ Given the pot odds (2.5:1) and your    â”‚            â”‚
â”‚    â”‚ position, calling is reasonable. However,â”‚            â”‚
â”‚    â”‚ consider 3-betting against loose        â”‚            â”‚
â”‚    â”‚ opponents to build the pot with premium â”‚            â”‚
â”‚    â”‚ equity.                          15:24   â”‚            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Threading Model

### Why Threading?

**Problem:** LLM inference takes 2-10 seconds
**Solution:** Run in background thread

```python
def _handle_chat_message(self, message: str):
    game_state = self.game.get_game_state()

    def stream_response():
        # Runs in background thread
        self.chat_panel.set_typing(True)

        full_response = ""
        for chunk in self.poker_advisor.get_advice_stream(message, game_state):
            full_response += chunk

        self.chat_panel.add_ai_response(full_response)

    # Start background thread
    thread = threading.Thread(target=stream_response, daemon=True)
    thread.start()

    # Main thread continues - game keeps running!
```

**Benefits:**
- Game runs at 60 FPS during LLM generation
- User can continue playing while waiting for advice
- Typing indicator shows advisor is "thinking"

---

## Performance Considerations

### Model Selection

**phi3:mini (2.2 GB)**
- **Inference speed:** 20-50 tokens/sec on CPU
- **Quality:** Good for poker advice
- **Memory:** 3-4 GB RAM during inference

**Alternatives:**
- `qwen2.5-coder:7b` - Better reasoning (slower)
- `deepseek-r1:7b` - Chain-of-thought (much slower)

### RAG Optimization

**Vector Store:**
- In-memory: Fast (0.01ms search)
- Pinecone: Slower (50-100ms) but scalable

**Chunking Strategy:**
- Chunk size: 500 chars
- Overlap: 100 chars
- Ensures complete concepts in each chunk

**Retrieval:**
- k=2 chunks (balance context vs prompt size)
- Cosine similarity threshold: 0.7
- Total context: ~1000 chars from knowledge base

---

## Configuration & Customization

### Change LLM Model

```python
# In poker_ev/llm/ollama_client.py
client = OllamaClient(model="qwen2.5-coder:7b")
```

### Adjust RAG Retrieval

```python
# In poker_ev/llm/poker_advisor.py
# Line 132: Change k value
rag_context = self.vector_store.search_as_context(user_query, k=3)  # More context
```

### Modify System Prompt

```python
# In poker_ev/llm/poker_advisor.py
# Lines 23-35: Edit SYSTEM_PROMPT
SYSTEM_PROMPT = """Your custom instructions here..."""
```

### Temperature Control

```python
# Higher = more creative, Lower = more deterministic
client = OllamaClient(temperature=0.9)  # More creative
client = OllamaClient(temperature=0.3)  # More focused
```

---

## Error Handling

### Ollama Not Running
```
Error: Cannot connect to Ollama
â†’ Check: ollama serve is running
â†’ Check: Model downloaded (ollama list)
```

### Out of Memory
```
Error: Ollama request timed out
â†’ Use smaller model (phi3:mini)
â†’ Close other applications
â†’ Reduce max_tokens parameter
```

### RAG Failures
```
Warning: Pinecone not initialized
â†’ Fallback: InMemoryPokerStore
â†’ No API key needed for in-memory mode
```

---

## Future Enhancements

### Planned Improvements

1. **Multi-turn Conversations**
   - Maintain chat history
   - Follow-up questions with context

2. **Advanced RAG**
   - Re-ranking retrieved chunks
   - Query expansion
   - Hybrid search (semantic + keyword)

3. **Performance**
   - Quantized models for faster inference
   - Caching common queries
   - Batch processing

4. **Features**
   - Hand history analysis
   - Opponent modeling from game data
   - Personalized advice based on play style

---

## References

### Code Locations

- **UI:** `poker_ev/gui/chat/`
- **LLM:** `poker_ev/llm/`
- **RAG:** `poker_ev/rag/`
- **Knowledge Base:** `poker_ev/rag/knowledge_base/`

### Dependencies

- **Ollama:** Local LLM runtime
- **sentence-transformers:** Embeddings
- **langchain:** RAG framework
- **pygame:** UI rendering
- **requests:** HTTP client

### External Documentation

- [Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)
- [Sentence Transformers](https://www.sbert.net/)

---

## Summary

The poker.ev RAG system combines:
1. **Real-time game context** - Current hand, position, odds
2. **Semantic knowledge retrieval** - Poker strategy from vector DB
3. **LLM generation** - Natural language advice with phi3:mini
4. **Streaming UI** - Responsive chat interface

All components work together to provide contextual, real-time poker advice while maintaining smooth gameplay at 60 FPS.
